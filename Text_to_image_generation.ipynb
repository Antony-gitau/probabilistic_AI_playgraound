{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3nF7WSwecDOlxjLETkPTL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antony-gitau/probabilistic_AI_playgraound/blob/main/Text_to_image_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following [MIT 6.S191 lecture](https://youtu.be/SA-v6Op2kL4?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) and taking some notes:\n",
        "\n",
        "examples of state of the art (SOTA) text to image generation tools include:\n",
        "- DALLE -2\n",
        "- imagen\n",
        "- muse\n",
        "\n",
        "techniques and applications:\n",
        "- diffusion models\n",
        "- autoregressive models\n",
        "\n",
        "image editing applications\n",
        "- fine tuning for instruction following. \n",
        "\n",
        "evaluation:\n",
        "- CLIP \n",
        "\n",
        "reconstruct masked tokens\n",
        "\n",
        "vector quantized latent space\n",
        "\n",
        "VQ GAN - Vextor quantized generative adversarial network.\n",
        "\n",
        "variable ratio masking.\n",
        "\n",
        "super-resolution model\n",
        "\n",
        "cross attention\n",
        "\n",
        "token based super resolution vs pixel based (diffusion) super resolution\n",
        "\n",
        "tricks for making the image look better.\n",
        "1. classifier free guidance at inference time \n",
        "- unconditional\n",
        "2. negative prompting\n",
        "3. iterative parellel decoding\n",
        "\n",
        "representing concepts in the language embedding space. this is the work of the large laguage model.\n",
        "then for text to image models, like muse, we are able to map the embedding to pixels. \n",
        "\n",
        "respect the semantic prompts of the text prompts. \n",
        "\n",
        "text based outpainting\n",
        "\n",
        "CLIP - Contrastive language image pretraining \n",
        "\n",
        "CLIP - confidence of learned image prevalence - assess the distribution of certain types of images within a dataset\n",
        "\n",
        "FID - Frechet inception distance. measure of quality and diversity of generated images in comparison to refence dataset."
      ],
      "metadata": {
        "id": "jL78-rrraw_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-emTp-K0ZOqy"
      },
      "outputs": [],
      "source": []
    }
  ]
}