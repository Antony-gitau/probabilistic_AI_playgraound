{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvbZVx2VcZTSWUIb1r7/uZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antony-gitau/probabilistic_AI_playgraound/blob/main/Variational_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Generative models aim to learn the probability density function $p(x)$ \n",
        " (distribution probability) that best describes the training data to enable generation by sampling from the distribution.\n",
        "\n",
        " - sample generation: \n",
        " - density estimation:\n",
        "\n",
        "different strategy to achieve this goal:\n",
        "1. explicit density model - \n",
        "2. implicit density model - \n",
        "\n",
        "latent variable models:\n",
        "1. variational autoencoder\n",
        "2. Generative Adversarial Network\n",
        "\n",
        "**Autoencoder**\n",
        "\n",
        "- unsupervised learning approach\n",
        "- dimensionality of latent space \n",
        "- representation learning \n",
        "- self encoding or auto-encoding\n",
        "\n",
        "**variational autoencoder**\n",
        "- probability twist on autoencoder\n",
        "- loss function: \n",
        "encoder: f\n",
        "\n",
        "\n",
        "\n",
        "generation is the process of computing a data point $x$ from the latent variable $z$. \n",
        "\n",
        "maximum likelihood of the mean or standard deviation - estimation technique for parameters of a probability distribution so that the distribution fits the observed data. \n",
        "\n",
        "\n",
        "likelihood - finding the optimal values for the mean (or standard deviation) for a distribution given a bunch of observed measurements.\n",
        "\n",
        "approximation methods:\n",
        "1. markov chain monte carlo\n",
        "2. variational inference\n",
        "\n",
        "\n",
        "amortized variational inference - train an external neural network to predict the variational parameters instead of optimizing ELBO per data point.\n",
        "\n",
        "reparametarization - rewriting the the expectation so that the distribution is independent of the parameter $Î˜$\n",
        "\n",
        "**GAN**\n",
        "- conditional GAN\n",
        "- CycleGAN\n",
        "\n",
        "**Diffusion models**\n",
        "\n",
        "[good read](https://theaisummer.com/latent-variable-models/)\n",
        "\n",
        "[mit lecture](https://youtu.be/3G5hWM6jqPk?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)\n",
        "\n",
        "In this notebook we will go into details on Variational Autoencoder with inspiration from [MIT'S Lab 2 on face detection](https://colab.research.google.com/drive/1j2W42n7R2DeBuO0xQZr7kDmTY2Z4Sexw#scrollTo=nLemS7dqECsI)\n",
        "\n",
        "![The concept of a VAE](https://i.ibb.co/3s4S6Gc/vae.jpg) this is the architectural design of a Variational Autoencoder (VAE).\n",
        "\n",
        "**encoder** transform inputs into variables, for example defined by mean and standard deviation, then draw from that distribution (defined by the mean and standard deviation) to generate a set of sampled latent variables.\n",
        "\n",
        "**decoder** the decoder's task is to reconstruct the latent variable to the original form of the input data. The decoder learns which latent variables are important. \n",
        "\n",
        "**goal of VAE** is therefore to train a model that learns a representation of the underlying latent space of the training data.\n",
        "\n",
        "**Loss function** \n",
        "- the encoder needs to learn latent variables which should ideally match the input (remember the input is transformed into a unit gaussian). this is the latent loss.\n",
        "\n",
        "- the decoder needs to reconstruct the latent variables clossly to the input. this is reconstruction loss.\n",
        "\n",
        "- so the total loss of a variational autoencoder is the sum of latent and reconstruction loss.\n"
      ],
      "metadata": {
        "id": "87ioSp15iAFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "YP-1pbtYXWGJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iXEbzH-Ch8xj"
      },
      "outputs": [],
      "source": [
        "def vae_loss_function(x, x_reconstructed, mu, logsigma, kl_weight = 0.005):\n",
        "  '''\n",
        "  we pass an input and its reconstruction.\n",
        "  we also define the prior distribution with mean and log of standard deviation\n",
        "  kl_weight is the weight parameter for the latent loss used for regularization\n",
        "  '''\n",
        "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) -1 - logsigma, axis=-1)\n",
        "  abs_diff = tf.abs(x, x_reconstructed)\n",
        "  reconstruction_loss = tf.reduce_mean(abs_diff, axis=[1,2,3])\n",
        "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
        "\n",
        "  return vae_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling and reparameterization"
      ],
      "metadata": {
        "id": "kKHYxZnbIHah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(z_mean, z_logsigma):\n",
        "  '''\n",
        "  we pass the mean and logarithm of variance of the learned latent variable\n",
        "  we extracts the batch size and the number of dimensions in the latent space\n",
        "  we then generate random samples from a standard normal distribution \n",
        "  the shape of the random sample is the same as the mean of latent variables\n",
        "  we compute the parameterization trick\n",
        "  '''\n",
        "  batch, latent_dim = z_mean.shape\n",
        "  epsilon = tf.random.normal(shape = (batch, latent_dim))\n",
        "  z = z_mean  + tf.exp(0.5 * z_logsigma) * epsilon\n",
        "\n",
        "  return z\n",
        "\n"
      ],
      "metadata": {
        "id": "aZC-4StWL-wY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semi-supervised variational autoencoder (SS-VAE)\n",
        "\n",
        "- the motivation is to use variational encoder to uncover bias in face recognition. \n",
        "- notice there is a supervised classification problem.\n",
        "- notice also that the VAE we talked about earlier did not output supervised variables.\n",
        "\n",
        "- lets workout a loss function"
      ],
      "metadata": {
        "id": "MeuJyhr5Q948"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ss_vae_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
        "  '''\n",
        "  '''\n",
        "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma, kl_weight = 0.005)\n",
        "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(y_logit,y)\n",
        "  classification_loss = "
      ],
      "metadata": {
        "id": "ej-ssxqhVsEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}